{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nltk.download(['punkt', 'punk_tab' 'stopwords', 'wordnet'])\n",
    "# nltk.download('punkt_tab')\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "# Preprocessing function (must be standalone and picklable)\n",
    "def preprocess_text(text, nlp):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalization\n",
    "    text = str(text).lower()\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r'@\\w+|http\\S+', '', text)\n",
    "    \n",
    "    # Cleaning (preserve punctuation)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenization and filtering\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 1]\n",
    "    \n",
    "    # Lemmatization with POS tagging\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    tokens = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your datasets\n",
    "train_df = pd.read_csv('data/train.csv')  # Replace with your actual train file\n",
    "test_df = pd.read_csv('data/test.csv')    # Replace with your actual test file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello cas fair anxiety depression work lot come work become much hard last year pretty good job waitress could cope high expectation overthought everything big give feedback know well end leave shift time wrong suppose start 11 thought 1 . freak badly answer call never go back time smoke plant cope really help give coming year work drive really anyone help 27 driving lesson expensive right want give work ago feel concerned long even get hire anywhere especially licence vicious cycle need licence car get job need job get licence car haha especially age feel optimistic able get back work gap resume centrelink work though many bill want get bit ahead life know go process manufacturing course july get forklift licence soon many job available hope would something easy would find something mostly worried manage anxiety work manage overthinke managing tell thing wrong concerned make mistake point get fire anyone something similar manage ok anyone able completely transparent workplace go type workplace hope find thank cas hi cas welcome detect underlie problem need tackle achieve download reliable worker thing like pay bill get drive lesson buy car fall place employer interested excuse people special need like we want worker problem free productive value pay wage manage reason forum popular people mind struggle place congregate share hide pain cafe employ physically disabled person disability would apparent flexibility would great experience 90 job 15 profession well separate work home life social life treatment apply job fork lift driver leave gap resume ask go overseas break work resume work job get job focus become reliable dealing issue mental illness outside work mean fill time sport hobby friend essentially expect much tolerance employer two part time job prefer fact distract well hope work google beyondblue topic never ever give beyondblue topic 30 minute save life tonywk hi cake feel empathise anxiety depression affect work work also mental health issue specific experience extent understand ... feel'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(train_df['content'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute stopwords as a global set\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Load spaCy in a thread-safe way\n",
    "class SpacyProcessor:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "        \n",
    "    def process(self, text):\n",
    "        return self.nlp(text)\n",
    "\n",
    "# Initialize one instance per worker\n",
    "spacy_processor = SpacyProcessor()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalization\n",
    "    text = str(text).lower()\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r'@\\w+|http\\S+', '', text)\n",
    "    \n",
    "    # Cleaning\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenization and filtering\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in STOPWORDS and len(t) > 1]\n",
    "    \n",
    "    # Lemmatization\n",
    "    doc = spacy_processor.process(\" \".join(tokens))\n",
    "    tokens = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Handle missing values\n",
    "train_df['content'] = train_df['content'].fillna('[MISSING]')\n",
    "test_df['content'] = test_df['content'].fillna('[MISSING]')\n",
    "\n",
    "# Parallel processing with thread-based backend\n",
    "def parallel_preprocess(df):\n",
    "    return Parallel(n_jobs=-1, backend='threading')(\n",
    "        delayed(preprocess_text)(text) for text in df['content']\n",
    "    )\n",
    "\n",
    "print(\"Preprocessing training data...\")\n",
    "train_df['cleaned_text'] = parallel_preprocess(train_df)\n",
    "\n",
    "print(\"Preprocessing test data...\")\n",
    "test_df['cleaned_text'] = parallel_preprocess(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store preprocessed text for later reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save results\n",
    "train_df[['id', 'cleaned_text', 'target']].to_csv('data/train_preprocessed.csv', index=False)\n",
    "test_df[['id', 'cleaned_text']].to_csv('data/test_preprocessed.csv', index=False)\n",
    "\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242, 25)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify missing content rows\n",
    "missing_train = train_df['content'].isna()\n",
    "missing_test = test_df['content'].isna()\n",
    "missing_train.sum(), missing_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = pd.read_csv('data/train_preprocessed.csv')\n",
    "new_test_df = pd.read_csv('data/test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22151, 3), (22151, 4))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_df.shape, train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing content with titles\n",
    "new_train_df.loc[missing_train, 'cleaned_text'] = train_df.loc[missing_train, 'title']\n",
    "new_test_df.loc[missing_test, 'cleaned_text'] = test_df.loc[missing_test, 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing imputation complete!\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "new_train_df.to_csv('data/train_preprocessed_fill_missing.csv', index=False)\n",
    "new_test_df.to_csv('data/test_preprocessed_fill_missing.csv', index=False)\n",
    "\n",
    "print(\"Missing imputation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
